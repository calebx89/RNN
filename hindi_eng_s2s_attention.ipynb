{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "hindi-eng-s2s-attention.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/calebx89/RNN/blob/main/hindi_eng_s2s_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "L_smeFXoA-qB"
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "AB-xu6mRA-qD"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "import re\n",
        "\n",
        "data = pd.read_csv(\"/kaggle/input/hindienglish-corpora/Hindi_English_Truncated_Corpus.csv\")\n",
        "english_sentences = data[\"english_sentence\"]\n",
        "hindi_sentences = data[\"hindi_sentence\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "LB6VzhezA-qE"
      },
      "source": [
        "len(english_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "RB1Uw5rJA-qE"
      },
      "source": [
        "# Global variables and Hyperparameters\n",
        "num_words = 35000 \n",
        "oov_token = '<UNK>'\n",
        "english_vocab_size = num_words + 1\n",
        "hindi_vocab_size = num_words + 1\n",
        "MAX_WORDS_IN_A_SENTENCE = 16\n",
        "test_ratio = 0.2\n",
        "BATCH_SIZE = 512\n",
        "embedding_dim = 64\n",
        "hidden_units = 1024\n",
        "learning_rate = 0.006\n",
        "epochs = 100\n",
        "\n",
        "def preprocess_sentence(sen, is_english):\n",
        "    if (type(sen) != str):\n",
        "        return ''\n",
        "    sen = sen.strip('.')\n",
        "\n",
        "    # insert space between words and punctuations\n",
        "    sen = re.sub(r\"([?.!,¿;।])\", r\" \\1 \", sen)\n",
        "    sen = re.sub(r'[\" \"]+', \" \", sen)\n",
        "    \n",
        "    # For english, replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\", \"'\")\n",
        "    if(is_english == True):\n",
        "        sen = re.sub(r\"[^a-zA-Z?.!,¿']+\", \" \", sen)\n",
        "        sen = sen.lower()\n",
        "\n",
        "    sen = sen.strip()\n",
        "    sen = 'sentencestart ' + sen + ' sentenceend'\n",
        "\n",
        "    sen = ' '.join(sen.split())\n",
        "    return sen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "LICAQw9MA-qF"
      },
      "source": [
        "# Loop through each datapoint having english and hindi sentence\n",
        "processed_e_sentences = []\n",
        "processed_h_sentences = []\n",
        "\n",
        "\n",
        "words_count = 0 \n",
        "for (e_sen, h_sen) in zip(english_sentences, hindi_sentences):\n",
        "    processed_e_sen = preprocess_sentence(e_sen, True)\n",
        "    processed_h_sen = preprocess_sentence(h_sen, False)\n",
        "    if(processed_e_sen == '' or processed_h_sen == '' or processed_e_sen.count(' ') > \\\n",
        "       (MAX_WORDS_IN_A_SENTENCE-1) or processed_h_sen.count(' ') > (MAX_WORDS_IN_A_SENTENCE-1)):\n",
        "        continue\n",
        "    #testing-----------\n",
        "    words_count+=1\n",
        "    if words_count> 3000:\n",
        "        break\n",
        "\n",
        "    processed_e_sentences.append(processed_e_sen)\n",
        "    processed_h_sentences.append(processed_h_sen)\n",
        "\n",
        "    \n",
        "print(\"Sentence examples: \")\n",
        "print(processed_e_sentences[0])\n",
        "print(processed_h_sentences[0])\n",
        "print(\"Length of English processed sentences: \" + str(len(processed_e_sentences)))\n",
        "print(\"Length of Hindi processed sentences: \" + str(len(processed_h_sentences)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "6-LE79l1A-qG"
      },
      "source": [
        "#finding num_words and MAX_WORDS_IN_A_SENTENCE\n",
        "\n",
        "vocabulary_e = {}\n",
        "wordcount_e = 0\n",
        "MAX_WORDS_IN_A_SENTENCE_e = 0\n",
        "\n",
        "for sen in processed_e_sentences:\n",
        "    words = sen.split()\n",
        "    if MAX_WORDS_IN_A_SENTENCE_e < len(words):\n",
        "        MAX_WORDS_IN_A_SENTENCE_e = len(words)\n",
        "    for word in words:\n",
        "        if word in vocabulary_e:\n",
        "            vocabulary_e[word]+=1\n",
        "        else:\n",
        "            vocabulary_e[word]=1\n",
        "            wordcount_e+=1\n",
        "print(wordcount_e)\n",
        "print(MAX_WORDS_IN_A_SENTENCE_e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "vbyzh9kRA-qG"
      },
      "source": [
        "\n",
        "def tokenize_sentences(processed_sentences, num_words, oov_token):\n",
        "    tokenizer = Tokenizer(num_words = num_words, oov_token = oov_token)\n",
        "    tokenizer.fit_on_texts(processed_sentences)\n",
        "    word_index = tokenizer.word_index\n",
        "    sequences = tokenizer.texts_to_sequences(processed_sentences)\n",
        "    sequences = pad_sequences(sequences, padding = 'post')\n",
        "    return word_index, sequences, tokenizer\n",
        "\n",
        "english_word_index, english_sequences, english_tokenizer = tokenize_sentences(processed_e_sentences, num_words, oov_token)\n",
        "hindi_word_index, hindi_sequences, hindi_tokenizer = tokenize_sentences(processed_h_sentences, num_words, oov_token)\n",
        "\n",
        "# split into traning and validation set\n",
        "english_train_sequences, english_val_sequences, hindi_train_sequences, hindi_val_sequences = train_test_split(english_sequences, hindi_sequences, test_size = test_ratio)\n",
        "BUFFER_SIZE = len(english_train_sequences)\n",
        "\n",
        "# Batching the training set\n",
        "dataset = tf.data.Dataset.from_tensor_slices((english_train_sequences, hindi_train_sequences)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder = True)\n",
        "print(\"No. of batches: \" + str(len(list(dataset.as_numpy_iterator()))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "DzK7gv8YA-qG"
      },
      "source": [
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, english_vocab_size, embedding_dim, hidden_units):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(english_vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(hidden_units, return_sequences = True, return_state = True)\n",
        "\n",
        "    def call(self, input_sequence):\n",
        "        x = self.embedding(input_sequence)\n",
        "        encoder_sequence_output, final_encoder_state = self.gru(x)\n",
        "        # Dimensions of encoder_sequence_output => (BATCH_SIZE, MAX_WORDS_IN_A_SENTENCE, hidden_units)\n",
        "        # Dimensions of final_encoder_state => (BATCH_SIZE, hidden_units)\n",
        "        return encoder_sequence_output, final_encoder_state\n",
        "\n",
        "# initialize our encoder\n",
        "encoder = Encoder(english_vocab_size, embedding_dim, hidden_units)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "P-e8xu89A-qH"
      },
      "source": [
        "\n",
        "class BasicDotProductAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(BasicDotProductAttention, self).__init__()\n",
        "\n",
        "    def call(self, decoder_hidden_state, encoder_outputs):\n",
        "        #Dimensions of decoder_hidden_state => (BATCH_SIZE, hidden_units)\n",
        "        #Dimensions of encoder_outputs => (BATCH_SIZE, MAX_WORDS_IN_A_SENTENCE, hidden_units)\n",
        "\n",
        "        decoder_hidden_state_with_time_axis = tf.expand_dims(decoder_hidden_state, 2)\n",
        "        #Dimensions of decoder_hidden_state_with_time_axis => (BATCH_SIZE, hidden_units, 1)\n",
        "        attention_scores = tf.matmul(encoder_outputs, decoder_hidden_state_with_time_axis)\n",
        "        #Dimensions of attention_scores => (BATCH_SIZE, MAX_WORDS_IN_A_SENTENCE, 1)\n",
        "        attention_scores = tf.nn.softmax(attention_scores, axis = 1)\n",
        "        weighted_sum_of_encoder_outputs = tf.reduce_sum(encoder_outputs * attention_scores, axis = 1)\n",
        "        #Dimensions of weighted_sum_of_encoder_outputs => (BATCH_SIZE, hidden_units)\n",
        "\n",
        "        return weighted_sum_of_encoder_outputs, attention_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "aAJfSqnzA-qI"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, hindi_vocab_size, embedding_dim, hidden_units):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(hindi_vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(hidden_units, return_state = True)\n",
        "        self.word_probability_layer = tf.keras.layers.Dense(hindi_vocab_size, activation = 'softmax')\n",
        "        self.attention_layer = BasicDotProductAttention()\n",
        "\n",
        "    def call(self, decoder_input, decoder_hidden, encoder_sequence_output):\n",
        "\n",
        "        x = self.embedding(decoder_input)\n",
        "        #Dimensions of x => (BATCH_SIZE, embedding_dim)\n",
        "        weighted_sum_of_encoder_outputs, attention_scores = self.attention_layer(decoder_hidden, encoder_sequence_output)\n",
        "        #Dimensions of weighted_sum_of_encoder_outputs => (BATCH_SIZE, hidden_units)\n",
        "        x = tf.concat([weighted_sum_of_encoder_outputs, x], axis = -1)\n",
        "        x = tf.expand_dims(x, 1)\n",
        "        #Dimensions of x => (BATCH_SIZE, 1, hidden_units + embedding_dim)\n",
        "        decoder_output, decoder_state = self.gru(x)\n",
        "        #Dimensions of decoder_output => (BATCH_SIZE, hidden_units)\n",
        "        word_probability = self.word_probability_layer(decoder_output)\n",
        "        #Dimensions of word_probability => (BATCH_SIZE, hindi_vocab_size)\n",
        "        return word_probability, decoder_state, attention_scores\n",
        "\n",
        "# initialize our decoder\n",
        "decoder = Decoder(hindi_vocab_size, embedding_dim, hidden_units)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Z72TB6veA-qI"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(reduction='none')\n",
        "def loss_function(actual_words, predicted_words_probability):\n",
        "    loss = loss_object(actual_words, predicted_words_probability)\n",
        "    mask = tf.where(actual_words > 0, 1.0, 0.0)\n",
        "    return tf.reduce_mean(mask * loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "d1AYcDj3A-qI"
      },
      "source": [
        "def train_step(english_sequences, hindi_sequences):\n",
        "    loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        encoder_sequence_output, encoder_hidden = encoder(english_sequences)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_input = hindi_sequences[:, 0]\n",
        "        for i in range(1, hindi_sequences.shape[1]):\n",
        "            predicted_words_probability, decoder_hidden, _ =\\\n",
        "                decoder(decoder_input, decoder_hidden, encoder_sequence_output)\n",
        "            actual_words = hindi_sequences[:, i]\n",
        "            # if all the sentences in batch are completed\n",
        "            if np.count_nonzero(actual_words) == 0:\n",
        "                break\n",
        "            loss += loss_function(actual_words, predicted_words_probability)\n",
        "\n",
        "            decoder_input = actual_words\n",
        "        #print(predicted_words_probability.shape)\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "    return loss.numpy()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "CpJwLiMIA-qJ"
      },
      "source": [
        "\n",
        "all_epoch_losses = []\n",
        "training_start_time = time.time()\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = []\n",
        "    start_time = time.time()\n",
        "    for(batch, (english_sequences, hindi_sequences)) in enumerate(dataset):\n",
        "        batch_loss = train_step(english_sequences, hindi_sequences)\n",
        "        epoch_loss.append(batch_loss)\n",
        "\n",
        "    all_epoch_losses.append(sum(epoch_loss)/len(epoch_loss))\n",
        "    print(\"Epoch No.: \" + str(epoch) + \" Time: \" + str(time.time()-start_time))\n",
        "\n",
        "print(\"All Epoch Losses: \" + str(all_epoch_losses))\n",
        "print(\"Total time in training: \" + str(time.time() - training_start_time))\n",
        "\n",
        "plt.plot(all_epoch_losses)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Epoch Loss\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}